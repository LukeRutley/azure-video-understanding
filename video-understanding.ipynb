{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc394ecc",
   "metadata": {},
   "source": [
    "# Video Analysis with Azure OpenAI\n",
    "\n",
    "This notebook demonstrates how to process video files frame-by-frame and analyze them using Azure OpenAI's GPT-4.1 vision capabilities.\n",
    "\n",
    "Azure offers a built-in video content understanding solution: [Azure Video Content Understanding](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/overview). \n",
    "\n",
    "**However, the Azure service has important technical constraints:**\n",
    "- **Frame sampling (~1 FPS):** The analyzer inspects about one frame per second. Rapid motions or single-frame events may be missed.\n",
    "- **Frame resolution (512 Ã— 512 px):** Sampled frames are resized to 512 pixels square. Small text or distant objects can be lost.\n",
    "\n",
    "This notebook avoids these limitations and provides much greater control over frame extraction rate, resolution, and analysis workflow.\n",
    "\n",
    "## Overview\n",
    "- Extract frames from a video file\n",
    "- Convert frames to base64 format for API consumption\n",
    "- Send frames to Azure OpenAI for analysis\n",
    "- Calculate the cost of processing\n",
    "\n",
    "## Prerequisites\n",
    "- Azure OpenAI resource with GPT-4.1 model deployed\n",
    "- Video file to analyze (e.g., `dash-cam-sample.mov`)\n",
    "- Required Python packages: `opencv-python`, `openai`, `azure-identity`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce70d8",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import the necessary libraries for video processing and Azure OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import base64\n",
    "import os\n",
    "import base64\n",
    "import cv2\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bcc74",
   "metadata": {},
   "source": [
    "## Extract Frames from Video\n",
    "\n",
    "**Note:** Make sure your video file is in the same directory as this notebook, or update the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0225a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired frame extraction rate (frames per second)\n",
    "target_fps = 1\n",
    "\n",
    "video = cv2.VideoCapture(\"dash-cam-sample.mov\")\n",
    "\n",
    "# Get video properties\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video FPS: {fps}\")\n",
    "\n",
    "frame_skip = max(1, int(fps // target_fps))\n",
    "\n",
    "base64Frames = []\n",
    "frame_count = 0\n",
    "\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # Only process every nth frame where n = frame_skip\n",
    "    if frame_count % frame_skip == 0:\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "video.release()\n",
    "print(f\"{len(base64Frames)} frames extracted (target: {target_fps} per second).\")\n",
    "print(f\"Total frames processed: {frame_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f17ca",
   "metadata": {},
   "source": [
    "## Analyze Video with Azure OpenAI\n",
    "\n",
    "Send the extracted frames to Azure OpenAI's GPT-4.1 model for analysis. The model will describe what happens in the video based on the frame sequence.\n",
    "\n",
    "### Configuration\n",
    "- **Model**: GPT-4.1 with vision capabilities\n",
    "- **Authentication**: Azure AD (Entra ID) using DefaultAzureCredential\n",
    "- **Image Detail**: Set to 'low' for cost efficiency\n",
    "\n",
    "### Environment Variables Required\n",
    "- `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI resource endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b305b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment = \"gpt-4.1\"\n",
    "detail = \"low\"\n",
    "\n",
    "# Initialize Azure OpenAI client with Entra ID authentication\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"These are frames from a video. Please describe what happens in the video.\"\n",
    "            },\n",
    "            *[\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{frame}\",\n",
    "                        \"detail\": {detail}\n",
    "                    }\n",
    "                }\n",
    "                for frame in base64Frames\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=messages,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31700677",
   "metadata": {},
   "source": [
    "## Cost Analysis\n",
    "\n",
    "Calculate the estimated cost of processing this video based on Azure OpenAI pricing.\n",
    "\n",
    "### Current Pricing (as of June 2025)\n",
    "- **Input tokens**: $2.00 per million tokens\n",
    "- **Output tokens**: $8.00 per million tokens\n",
    "\n",
    "**Reference**: [Azure OpenAI Pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)\n",
    "\n",
    "### Cost Factors\n",
    "- Number of frames sent\n",
    "- Image detail level (low/high)\n",
    "- Length of text prompt\n",
    "- Length of generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a77f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = completion.usage.prompt_tokens\n",
    "output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "input_cost = input_tokens / 1_000_000 * 2  # $2 per million input tokens\n",
    "output_cost = output_tokens / 1_000_000 * 8  # $8 per million output tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"Token Usage:\")\n",
    "print(f\"  Input tokens: {input_tokens:,}\")\n",
    "print(f\"  Output tokens: {output_tokens:,}\")\n",
    "print(f\"  Total tokens: {input_tokens + output_tokens:,}\")\n",
    "print(f\"\")\n",
    "print(f\"Cost Breakdown:\")\n",
    "print(f\"  Input cost: ${input_cost:.6f}\")\n",
    "print(f\"  Output cost: ${output_cost:.6f}\")\n",
    "print(f\"  Total cost: ${total_cost:.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"Cost per frame: ${total_cost/len(base64Frames):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a206e51",
   "metadata": {},
   "source": [
    "### Use Cases\n",
    "- **Security**: Analyze surveillance footage\n",
    "- **Insurance**: Process dash cam footage for claims\n",
    "- **Content Moderation**: Review video content\n",
    "- **Research**: Analyze behavioral patterns in video data\n",
    "\n",
    "### Performance Tips\n",
    "- Use `\"detail\": \"low\"` for faster, cheaper processing\n",
    "- Use `\"detail\": \"high\"` if you need more precise analysis (e.g., for fine-grained object detection or scene understanding), but note this increases cost and latency\n",
    "- Adjust frame sampling rate based on video content\n",
    "- Experiment with different frame sampling rates\n",
    "- Customize prompts for specific use cases (e.g., object detection, activity recognition)\n",
    "- Use structured outputs if integrating into software/pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
